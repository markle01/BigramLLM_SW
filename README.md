# GPT project

<a id="readme-top"></a>
[![Contributors][contributors-shield]][contributors-url]

<br />
<div align="center">
  <a href="https://victorzhou.com/media/nn-series/network.png">
    <img src="https://victorzhou.com/media/nn-series/network.png" alt="Logo" width="80" height="80">
  </a>

  <h3 align="center">Bigram Language Model</h3>
</div>

<!-- ABOUT THE PROJECT -->
## About The Project

This project aims to develop a robust language model through a series of incremental improvements. Each milestone focuses on enhancing the model's capabilities, from basic bigram language modeling to advanced techniques like multi-head attention and dropout. The project is structured to ensure systematic progress and thorough evaluation at each stage.

### Milestones
#### Milestone 1: Dataset Exploration and Preparation
<ul> Objective: Select and prepare a dataset for training the language model. </ul> 
<ul> Tasks: 
  <li>Select a Dataset: Choose a dataset that suits your project goals, such as movie scripts, Reddit comments, or books.</li> 
  <li>Clean and Tokenize the Data: Process the raw text data to remove noise (e.g., special characters, extra spaces) and tokenize it into meaningful units (e.g., words or characters).</li> 
  <li>Justify Dataset Choices: Write a project report explaining why you chose this dataset, including its relevance and potential impact on the model's performance.</li> 
</ul>

#### Milestone 2: Basic Model Usage (Bigram Language Model)
<ul> Objective: Implement and train a basic Bigram Language Model. </ul> 
<ul> Tasks: 
  <li>Use the Provided Model: Start with the provided Bigram Language Model.</li> 
  <li>Train the Model: Train the model on the selected dataset.</li> 
  <li>Track Performance Metrics: Monitor metrics such as training loss and validation loss during training.</li> 
  <li>Print Metrics: Print out average training loss and validation loss at the end of training.</li> 
  <li>Generate Text: Print out generated tokens/text to preview the model's current state.</li> 
  <li>Save Generated Text: Save generated text of 300 - 500 tokens to a file called <code>milestone2.txt</code>.</li> 
</ul>

#### Milestone 3: Self-attention & Softmax Iteration
<ul> Objective: Enhance the model with self-attention mechanisms. </ul> 
<ul> Tasks: 
  <li>Update the Model: Incorporate self-attention mechanisms into the model.</li> 
  <li>Train the Model: Train the updated model on the dataset.</li> 
  <li>Track Performance Metrics: Monitor metrics such as training loss and validation loss during training.</li> 
  <li>Print Metrics: Print out average training loss and validation loss at the end of training.</li> 
  <li>Generate Text: Print out generated tokens/text to preview the model's current state.</li> 
  <li>Save Generated Text: Save generated text of 300 - 500 tokens to a file called <code>milestone3.txt</code>.</li> 
</ul>

#### Milestone 4: Multi-head Attention
<ul> Objective: Implement multi-head attention in the model. </ul> 
<ul> Tasks: 
  <li>Update the Model: Add multi-head attention mechanisms to the model.</li> 
  <li>Train the Model: Train the updated model on the dataset.</li> 
  <li>Track Performance Metrics: Monitor metrics such as training loss and validation loss during training.</li> 
  <li>Print Metrics: Print out average training loss and validation loss at the end of training.</li> 
  <li>Generate Text: Print out generated tokens/text to preview the model's current state.</li> 
  <li>Save Generated Text: Save generated text of 300 - 500 tokens to a file called <code>milestone4.txt</code>.</li> 
</ul>

#### Milestone 5: Feed Forward Layers
<ul> Objective: Integrate feed forward layers into the model. </ul> 
<ul> Tasks: 
  <li>Update the Model: Add feed forward layers to the model.</li> 
  <li>Train the Model: Train the updated model on the dataset.</li> 
  <li>Track Performance Metrics: Monitor metrics such as training loss and validation loss during training.</li> 
  <li>Print Metrics: Print out average training loss and validation loss at the end of training.</li> 
  <li>Generate Text: Print out generated tokens/text to preview the model's current state.</li> 
  <li>Save Generated Text: Save generated text of 300 - 500 tokens to a file called <code>milestone5.txt</code>.</li> 
</ul>

#### Milestone 6: Residual Connections
<ul> Objective: Incorporate residual connections into the model. </ul> 
<ul> Tasks: 
  <li>Update the Model: Add residual connections to the model to improve gradient flow and training stability.</li> 
  <li>Train the Model: Train the updated model on the dataset.</li> 
  <li>Track Performance Metrics: Monitor metrics such as training loss and validation loss during training.</li> 
  <li>Print Metrics: Print out average training loss and validation loss at the end of training.</li> 
  <li>Generate Text: Print out generated tokens/text to preview the model's current state.</li> 
  <li>Save Generated Text: Save generated text of 300 - 500 tokens to a file called <code>milestone6.txt</code>.</li> 
</ul>

#### Milestone 7: Layer Normalization
<ul> Objective: Implement layer normalization in the model. </ul> 
<ul> Tasks: 
  <li>Update the Model: Add layer normalization to the model to stabilize and accelerate training.</li> 
  <li>Train the Model: Train the updated model on the dataset.</li> 
  <li>Track Performance Metrics: Monitor metrics such as training loss and validation loss during training.</li> 
  <li>Print Metrics: Print out average training loss and validation loss at the end of training.</li> 
  <li>Generate Text: Print out generated tokens/text to preview the model's current state.</li> 
  <li>Save Generated Text: Save generated text of 300 - 500 tokens to a file called <code>milestone7.txt</code>.</li> 
</ul>

#### Milestone 8: Dropout
<ul> Objective: Integrate dropout into the model to prevent overfitting. </ul> 
<ul> Tasks: 
  <li>Update the Model: Add dropout layers to the model.</li> 
  <li>Train the Model: Train the updated model on the dataset.</li> 
  <li>Track Performance Metrics: Monitor metrics such as training loss and validation loss during training.</li> 
  <li>Print Metrics: Print out average training loss and validation loss at the end of training.</li> 
  <li>Generate Text: Print out generated tokens/text to preview the model's current state.</li> 
  <li>Save Generated Text: Save generated text of 300 - 500 tokens to a file called <code>milestone8.txt</code>.</li>
</ul>

This structured approach ensures that each enhancement is thoroughly tested and evaluated, leading to a progressively more sophisticated and capable language model.
<!-- MARKDOWN LINKS & IMAGES -->
[contributors-shield]: https://img.shields.io/github/contributors/othneildrew/Best-README-Template.svg?style=for-the-badge
[contributors-url]: https://github.com/markle01/project640/graphs/contributors
